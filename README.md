# dbt + Airflow + Snowflake ETL Pipeline

Un proyecto completo de ETL usando dbt, Apache Airflow y Snowflake con el dataset de muestra TPCH para demostrar mejores pr√°cticas en ingenier√≠a de datos.

## Arquitectura del Proyecto

```mermaid
graph LR
    A[Snowflake TPCH Dataset] --> B[dbt Staging Models]
    B --> C[dbt Mart Models]
    C --> D[Snowflake Data Warehouse]
    E[Apache Airflow] --> F[Cosmos dbt Operator]
    F --> B
    G[Astronomer] --> E
```

## Tecnolog√≠as Utilizadas

- **Orquestaci√≥n**: Apache Airflow + Astronomer
- **Transformaci√≥n**: dbt (data build tool)
- **Data Warehouse**: Snowflake
- **Containerizaci√≥n**: Docker
- **Framework de DAG**: Astronomer Cosmos

## Estructura del Proyecto

```
dbt-dag/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ dbt-dag.py                 # DAG principal de Airflow
‚îÇ   ‚îî‚îÄ‚îÄ dbt/
‚îÇ       ‚îî‚îÄ‚îÄ data_pipeline/         # Proyecto dbt
‚îÇ           ‚îú‚îÄ‚îÄ dbt_project.yml    # Configuraci√≥n dbt
‚îÇ           ‚îú‚îÄ‚îÄ models/
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ staging/       # Modelos de limpieza inicial
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ marts/         # Modelos de negocio
‚îÇ           ‚îú‚îÄ‚îÄ macros/            # Funciones reutilizables
‚îÇ           ‚îú‚îÄ‚îÄ tests/             # Tests de calidad de datos
‚îÇ           ‚îî‚îÄ‚îÄ docs/              # Documentaci√≥n
‚îú‚îÄ‚îÄ include/                       # Archivos auxiliares
‚îú‚îÄ‚îÄ requirements.txt               # Dependencias Python
‚îú‚îÄ‚îÄ packages.txt                   # Paquetes del sistema
‚îú‚îÄ‚îÄ airflow_settings.yaml         # Configuraciones locales
‚îî‚îÄ‚îÄ Dockerfile                     # Imagen Docker personalizada
```

## Configuraci√≥n del Entorno

### Prerrequisitos

- Docker Desktop
- Astronomer CLI
- Cuenta de Snowflake
- Python 3.8+

### Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone https://github.com/Cesar7234/dbt-dag.git
cd dbt-dag
```

2. **Instalar Astronomer CLI**
```bash
# macOS
brew install astronomer/tap/astro

# Linux
curl -sSL install.astronomer.io | sudo bash -s
```

3. **Configurar variables de entorno**
```bash
cp .env.example .env
# Editar .env con tus credenciales de Snowflake
```

4. **Iniciar el proyecto**
```bash
astro dev start
```

## Configuraci√≥n de Snowflake

### 1. Crear Recursos Necesarios

```sql
-- Crear database y schema
CREATE DATABASE IF NOT EXISTS dbt_db;
CREATE SCHEMA IF NOT EXISTS dbt_db.dbt_schema;

-- Verificar acceso al dataset TPCH
USE WAREHOUSE COMPUTE_WH;
SELECT COUNT(*) FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS;
```

### 2. Configurar Conexi√≥n en Airflow

En la interfaz de Airflow (http://localhost:8080):

**Admin > Connections > Create**

- **Connection ID**: `snowflake_conn`
- **Connection Type**: `Snowflake`
- **Host**: `tu-account.snowflakecomputing.com`
- **Schema**: `dbt_schema`
- **Login**: `tu_usuario`
- **Password**: `tu_contrase√±a`
- **Extra**:
```json
{
  "account": "tu-account-identifier",
  "warehouse": "COMPUTE_WH",
  "database": "dbt_db",
  "role": "SYSADMIN"
}
```

## Modelos de Datos

### Staging Layer
- `stg_tpch_orders`: Limpieza y tipado de √≥rdenes
- `stg_tpch_customers`: Normalizaci√≥n de clientes
- `stg_tpch_lineitems`: Procesamiento de l√≠neas de pedido

### Marts Layer
- `dim_customers`: Dimensi√≥n de clientes
- `fct_orders`: Tabla de hechos de √≥rdenes
- `daily_sales_summary`: M√©tricas agregadas diarias

## Pipeline de Datos

El DAG ejecuta las siguientes tareas:

1. **Validaci√≥n de conexiones**
2. **Instalaci√≥n de dependencias dbt** (`dbt deps`)
3. **Ejecuci√≥n de tests de sources** (`dbt test --select source:*`)
4. **Transformaciones staging** (`dbt run --select staging.*`)
5. **Tests de staging** (`dbt test --select staging.*`)
6. **Transformaciones marts** (`dbt run --select marts.*`)
7. **Tests finales** (`dbt test --select marts.*`)
8. **Generaci√≥n de documentaci√≥n** (`dbt docs generate`)

![dbt_dag-graph](dbt_dag-graph.png)

## Monitoreo y Observabilidad

### M√©tricas Clave
- Tiempo de ejecuci√≥n por modelo
- Cantidad de registros procesados
- Tests fallidos/exitosos
- Uso de recursos de Snowflake

### Alertas
- Fallos en la pipeline
- Tests de calidad fallidos
- Ejecuciones que exceden el SLA

## Testing

### Tests de dbt Incluidos

```yaml
# models/staging/schema.yml
version: 2

models:
  - name: stg_tpch_orders
    columns:
      - name: order_key
        tests:
          - unique
          - not_null
      - name: order_date
        tests:
          - not_null
```

### Ejecutar Tests Localmente

```bash
# Dentro del contenedor de dbt
dbt test --profiles-dir ./
dbt test --select stg_tpch_orders
```

## Deployment

### Desarrollo Local
```bash
astro dev start
```

### Producci√≥n (Astronomer)
```bash
# Configurar deployment
astro auth login
astro deployment list

# Deploy
astro deploy <deployment-id>
```

## Documentaci√≥n

### Generar Docs de dbt
```bash
dbt docs generate --profiles-dir ./
dbt docs serve --profiles-dir ./
```

Visitar: http://localhost:8080/docs

## üîß Troubleshooting

### Errores Comunes

1. **Connection Error**
   - Verificar credenciales de Snowflake
   - Confirmar account identifier correcto

2. **Model Compilation Error**
   - Revisar sintaxis SQL en modelos
   - Verificar referencias a sources/models

3. **Permission Denied**
   - Confirmar permisos en Snowflake
   - Verificar role y warehouse

### Debug Commands

```bash
# Validar configuraci√≥n
dbt debug --profiles-dir ./

# Compilar modelos sin ejecutar
dbt compile --profiles-dir ./

# Ver logs detallados
astro dev logs -f
```

## Contribuciones

1. Fork el proyecto
2. Crear feature branch (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push branch (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## Licencia

Este proyecto est√° bajo la licencia MIT. Ver `LICENSE` para m√°s detalles.

## Contacto

- **Autor**: C√©sar Rangel
- **GitHub**: [@Cesar7234](https://github.com/Cesar7234)
- **LinkedIn**: https://www.linkedin.com/in/cesar-rangel-sosa/

## Reconocimientos

- [Astronomer](https://astronomer.io/) por la plataforma de Airflow
- [dbt Labs](https://www.getdbt.com/) por la herramienta de transformaci√≥n
- [Snowflake](https://snowflake.com/) por el data warehouse
- Comunidad open source de data engineering